{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "neg=pd.read_csv('../data/neg.csv',header=None,index_col=None)\n",
    "pos=pd.read_csv('../data/pos.csv',header=None,index_col=None,on_bad_lines=\"skip\")\n",
    "neu=pd.read_csv('../data/neutral.csv',header=None,index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       做为一本声名在外的流行书，说的还是广州的外企，按道理应该和我的生存环境差不多啊。但是一看之下...\n",
       "1       作者完全是以一个过来的自认为是成功者的角度去写这个问题，感觉很不客观。虽然不是很喜欢，但是，...\n",
       "2            作者提倡内调，不信任化妆品，这点赞同。但是所列举的方法太麻烦，配料也不好找。不是太实用。\n",
       "3       作者的文笔还行，但通篇感觉太琐碎，有点文人的无病呻吟。自由主义者。作者的品性不敢苟同，无民族...\n",
       "4                           作者倒是个很小资的人,但有点自恋的感觉,书并没有什么大帮助\n",
       "                              ...                        \n",
       "4350                              虽然家里用不到 但商家的服务态度 超级棒！赞！\n",
       "4351                  已经安装上了，但没有试，我相信美的质量，应该不会有问题。先5分好评吧。\n",
       "4352    很不错，到货很快，当天给客服打电话，下午安装人员就上门了，速度啊!但但当时没有水，没有试试，...\n",
       "4353    买来放在出租房里的，所以自己也没试过，但是安装服务人员特别好，最大限度地给省钱，两套热水澡装...\n",
       "4354    买来放在出租房里的，所以自己也没试过，但是安装服务人员特别好，最大限度地给省钱，两套热水澡装...\n",
       "Name: 0, Length: 4355, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       作者有明显的自恋倾向，只有有老公养不上班的太太们才能像她那样生活。很多方法都不实用，还有抄袭...\n",
       "1                       作者的文笔一般，观点也是和市面上的同类书大同小异，不推荐读者购买。\n",
       "2       作为一本描写过去年代感情生活的小说，作者明显生活经验不足，并且文字功底极其一般，看后感觉浪费...\n",
       "3                              昨天打开书才翻了几页，书页纷纷掉落了，请问怎么回事？\n",
       "4       最近下单号为：1442083355，其中一套书《易经的智慧》当当网  少带光盘 。为了得到解...\n",
       "                              ...                        \n",
       "8698           安装我自己花了500多，美的够黑心的，真的是烦心，安装的售后叼的要死！差评！！！！！\n",
       "8699                东西不错，售后太差，安装一个热水器400块钱，像话吗？钱给完了发现被黑了！\n",
       "8700    碰到最差的、最骗人的卖家，好吧，我倒霉！这个卖家太不厚道了，显示所在地上海，我买这个热水器选...\n",
       "8701                                      宝贝不错，物流也不错，售后差，\n",
       "8702    美的售后太垃圾，其他售后都是两小时回电话，美的是24小时，结果超过市区不到五公里问我收五十，...\n",
       "Name: 0, Length: 8703, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...\n",
       "1       作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...\n",
       "2       作者用诗一样的语言把如水般清澈透明的思想娓娓道来，像一个经验丰富的智慧老人为我们解开一个又一...\n",
       "3       作者提出了一种工作和生活的方式，作为咨询界的元老，不仅能提出理念，而且能够身体力行地实践，并...\n",
       "4       作者妙语连珠，将整个60-70年代用层出不穷的摇滚巨星与自身故事紧紧相连什么是乡愁？什么是摇...\n",
       "                              ...                        \n",
       "8025                                                   好评\n",
       "8026                                                   好评\n",
       "8027                                                   好评\n",
       "8028                                                   好评\n",
       "8029                                                   好评\n",
       "Name: 0, Length: 8030, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21088,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combined = np.concatenate((pos[0], neu[0], neg[0]))\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21088,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos -> 1; neu -> 0; neg -> -1\n",
    "y = np.concatenate((np.ones(len(pos), dtype=int), np.zeros(len(neu), dtype=int), -1*np.ones(len(neg),dtype=int)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\64056\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.907 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "#对句子经行分词，并去掉换行符\n",
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text\n",
    "\n",
    "combined = tokenizer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Training a Word2vec model...\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "import multiprocessing\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count() # 4\n",
    "vocab_dim = 100\n",
    "n_iterations = 10  # ideally more..\n",
    "n_exposures = 10 # 所有频数超过10的词语\n",
    "window_size = 7\n",
    "n_epoch = 4\n",
    "input_length = 100\n",
    "maxlen = 100\n",
    "\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.index_to_key,\n",
    "                            allow_update=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        #  freqxiao10->0 所以k+1\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引,(k->v)=>(v->k)\n",
    "        \n",
    "        w2vec = {word: model.wv[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量, (word->model(word))\n",
    "        #w2vec = {word: model[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量, (word->model(word))\n",
    "\n",
    "        \n",
    "        \n",
    "        def parse_dataset(combined): # 闭包-->临时使用\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0) # freqxiao10->0\n",
    "                data.append(new_txt)\n",
    "            return data # word=>index\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\n",
    "def word2vec_train(combined):\n",
    "\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     epochs=n_iterations)\n",
    "    model.build_vocab(combined) # input: list\n",
    "    \n",
    "    #index_dict, word_vectors,combined=word2vec_train(combined,total_examples=model.corpus_count,epochs=n_iterations)\n",
    "    \n",
    "    model.train(combined,total_examples=model.corpus_count,epochs=n_iterations)\n",
    "    #model.train(combined)\n",
    "    model.save('../model/Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "#index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "#index_dict, word_vectors,combined=word2vec_train(combined,total_examples=model.corpus_count,epochs=n_iterations)\n",
    "\n",
    "index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "\n",
    "print(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n",
      "x_train and y_train shapes:\n",
      "(16870, 100) (16870, 3)\n",
      "Defining a Simple Keras Model...\n",
      "Compiling the Model...\n",
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Train...\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "528/528 [==============================] - 28s 47ms/step - loss: 0.7969 - accuracy: 0.7523\n",
      "Epoch 2/4\n",
      "528/528 [==============================] - 25s 47ms/step - loss: 0.6639 - accuracy: 0.8892\n",
      "Epoch 3/4\n",
      "528/528 [==============================] - 25s 48ms/step - loss: 0.6407 - accuracy: 0.9118\n",
      "Epoch 4/4\n",
      "528/528 [==============================] - 24s 46ms/step - loss: 0.6297 - accuracy: 0.9226\n",
      "Evaluate...\n",
      "132/132 [==============================] - 3s 13ms/step - loss: 0.6507 - accuracy: 0.8988\n",
      "loss:  0.6507382392883301\n",
      "accuracy:  0.8987671732902527\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import model_from_yaml\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "import yaml\n",
    "import keras\n",
    "from tensorflow.keras.layers import Flatten\n",
    "batch_size = 32\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim)) # 初始化 索引为0的词语，词向量全为0\n",
    "    for word, index in index_dict.items(): # 从索引为1的词语开始，对每个词语对应其词向量\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    y_train = keras.utils.to_categorical(y_train,num_classes=3) \n",
    "    y_test = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "    # print x_train.shape,y_train.shape\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test\n",
    "\n",
    "\n",
    "\n",
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print('Defining a Simple Keras Model...')  \n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(input_dim=n_symbols,\n",
    "                      output_dim=vocab_dim,  # Embedding dimension\n",
    "                      mask_zero=True,  # Mask zero paddings\n",
    "                      weights=[embedding_weights],\n",
    "                      input_length=input_length))\n",
    "    model.add(LSTM(units=50, activation='tanh', return_sequences=True))  # Allows stacking more LSTMs\n",
    "\n",
    "    #model.add(LSTM(units=50, activation='tanh', inner_activation='hard_sigmoid'))\n",
    "\n",
    "    #model.add(LSTM(output_dim=50, activation='tanh', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(Dense(3, activation='softmax')) # Dense=>全连接层,输出维度=1\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print('Compiling the Model...')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print(\"Train...\") # batch_size=32\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch,verbose=1)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Evaluate...\")\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "  # Save model architecture and weights\n",
    "    model_json = model.to_json()\n",
    "    with open('../model/lstm.json', 'w') as outfile:\n",
    "        outfile.write(model_json)\n",
    "    model.save_weights('../model/lstm.h5')\n",
    "    print('loss: ', loss)\n",
    "    print('accuracy: ',accuracy)\n",
    "\n",
    "    return model  # Optional: return the trained model\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Get data from your function (assuming it returns all necessary data)\n",
    "print('Setting up Arrays for Keras Embedding Layer...')\n",
    "n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(index_dict, word_vectors, combined, y)\n",
    "\n",
    "print(\"x_train and y_train shapes:\")  # Clearer message\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test)\n",
    "\n",
    "#print(111)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "预测\n",
    "\"\"\"\n",
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import json  # Assuming you're using json for model architecture\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import yaml\n",
    "from keras.models import model_from_yaml\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "import sys\n",
    "sys.setrecursionlimit(1000000)\n",
    "\n",
    "# define parameters\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionaries(model=None):\n",
    "    \"\"\"\n",
    "    Creates a word-to-index mapping based on the provided Word2Vec model.\n",
    "\n",
    "    Args:\n",
    "        model (Optional[Word2Vec]): The Word2Vec model to use for vocabulary extraction.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their corresponding indices.\n",
    "    \"\"\"\n",
    "\n",
    "    if model is not None:\n",
    "        w2indx = {v: k + 1 for k, v in enumerate(model.wv.vocab)}\n",
    "        return w2indx\n",
    "    else:\n",
    "        print('No Word2Vec model provided for dictionary creation.')\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)\n",
    "\n",
    "        \n",
    "        #  freqxiao10->0 所以k+1\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}#所有频数超过10的词语的索引,(k->v)=>(v->k)\n",
    "        w2vec = {word: model.wv[word] for word in w2indx.keys()}#所有频数超过10的词语的词向量, (word->model(word))\n",
    "\n",
    "        def parse_dataset(combined): # 闭包-->临时使用\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0) # freqxiao10->0\n",
    "                data.append(new_txt)\n",
    "            return data # word=>index\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)#每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print('No data provided...')\n",
    "\n",
    "\n",
    "# def input_transform(string):\n",
    "#     words=jieba.lcut(string)\n",
    "#     words=np.array(words).reshape(1,-1)\n",
    "#     model=Word2Vec.load('../model/Word2vec_model.pkl')\n",
    "#     _,_,combined=create_dictionaries(model,words)\n",
    "#     return combined\n",
    "\n",
    "\n",
    "\n",
    "def input_transform(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    model=Word2Vec.load('../model/Word2vec_model.pkl')\n",
    "    _,_,combined=create_dictionaries(model,words)\n",
    "    return combined\n",
    "\n",
    "\n",
    "\n",
    "def lstm_predict(string):\n",
    "    print('loading model......')\n",
    "    json_file = '../model/lstm.json'\n",
    "    with open(json_file, 'r') as f:\n",
    "        model_json = f.read()\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "\n",
    "    print('loading weights......')\n",
    "    model.load_weights('../model/lstm.h5')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    data=input_transform(string)\n",
    "    data.reshape(1,-1)\n",
    "    #print data\n",
    "    \n",
    "    predictions=model.predict(data)\n",
    "    #rint(predictions) # [[1]]\n",
    "    \n",
    "    \n",
    "    result = np.argmax(predictions, axis=-1)[0]\n",
    "    \n",
    "    \n",
    "#     result=model.predict(data)[0]\n",
    "#     print(result) # [[1]]\n",
    "    \n",
    "\n",
    "    if result==1:\n",
    "        print(string,' positive')\n",
    "    elif result==0:\n",
    "        print(string,' neural')\n",
    "    else:\n",
    "        print(string,' negative')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model......\n",
      "loading weights......\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "非常好非常好！！  positive\n",
      "loading model......\n",
      "loading weights......\n",
      "1/1 [==============================] - 1s 917ms/step\n",
      "真的一般，没什么可以学习的  negative\n"
     ]
    }
   ],
   "source": [
    "# string='酒店的环境非常好，价格也便宜，值得推荐'\n",
    "# string='手机质量太差了，傻逼店家，赚黑心钱，以后再也不会买了'\n",
    "# string = \"这是我看过文字写得很糟糕的书，因为买了，还是耐着性子看完了，但是总体来说不好，文字、内容、结构都不好\"\n",
    "# string = \"虽说是职场指导书，但是写的有点干涩，我读一半就看不下去了！\"\n",
    "# string = \"书的质量还好，但是内容实在没意思。本以为会侧重心理方面的分析，但实际上是婚外恋内容。\"\n",
    "#string = \"不是太好\"\n",
    "# string = \"不错不错\"\n",
    "string_1 = \"非常好非常好！！\"\n",
    "string_2 = \"真的一般，没什么可以学习的\"\n",
    "\n",
    "\n",
    "lstm_predict(string_1)\n",
    "lstm_predict(string_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5.4\n"
     ]
    }
   ],
   "source": [
    "! jupyter notebook --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "#输出数据到CSV文件\n",
    "import csv #调用数据保存文件\n",
    "import pandas as pd #用于数据输出\n",
    "#一个sheet\n",
    "list1=[1,2]\n",
    "list2=[3,4]\n",
    "list=[]\n",
    "list.append(list1)\n",
    "list.append(list2)\n",
    "print(list)\n",
    "column=['one','two'] #列表头名称\n",
    "test=pd.DataFrame(columns=column,data=list)#将数据放进表格\n",
    "test.to_csv('test.csv') #数据存入csv,存储位置及文件名称\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
